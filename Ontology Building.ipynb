{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2a0f80-57af-4385-9414-d9f4f56f2bda",
   "metadata": {},
   "source": [
    "# Ontology Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80326047-41e8-4658-a281-e158db7048f4",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e382cb-f0da-4e81-aa42-47b6646b7003",
   "metadata": {},
   "source": [
    " The utils.py file includes the relevant imports and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908cfbe-d22d-4751-a78c-89fa7cb7099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af350a-c4b4-4e1b-92ad-c0b22a7bd891",
   "metadata": {},
   "source": [
    "Define file paths for output and data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9bf7f-0d2d-4a58-b412-6b147fa09d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"  # You must define this path.\n",
    "out_path = \"./output\"  # Path to the directory for saving output files.\n",
    "\n",
    "# Make sure you created the following files using coreference resolution algorithm output and the cleaning script:\n",
    "weighted_pairs_file = \"weighted_pairs.csv\"  # Graph edges\n",
    "phrase2clean_file = \"phrase2clean.pkl\"  # A dict containing the clean phrases of their original occurences\n",
    "\n",
    "phrase_occurences_path = os.path.join(data_path, weighted_pairs_file)  \n",
    "phrase2clean_path = os.path.join(data_path, phrase2clean_file)  \n",
    "\n",
    "betweeness_path = os.path.join(data_path, \"betweenness_full_graph_#k=500.json\")  # Optional - run betweeeness centrality separately (may take some time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2bd461-f9f9-42d2-9209-a7be0f7e2967",
   "metadata": {},
   "source": [
    "## 1. Coreference Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e5c25-16f4-4144-86b5-68ab3577089b",
   "metadata": {},
   "source": [
    "Load graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da027e90-76e5-42d0-876c-a4bef4c08587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(phrase_occurences_path):\n",
    "    print(\"missing file!\", phrase_occurences_path)\n",
    "else:\n",
    "    df = pd.read_csv(phrase_occurences_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2f8c6-d525-4e05-9737-31ea365244ba",
   "metadata": {},
   "source": [
    "Create a weighted undirected graph from the edges in 'df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eda16b-19f5-4860-b0e5-bdf18088e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for index, row in df.iterrows():\n",
    "    edges.append((row['Node1'], row['Node2'], {\"weight\": row[\"Weight\"]}))\n",
    "\n",
    "G_w = nx.Graph()\n",
    "G_w.add_edges_from(edges)  \n",
    "G_w.number_of_nodes(), G_w.number_of_edges()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2c719-21c0-41fa-8238-90001bbd0a84",
   "metadata": {},
   "source": [
    "## 2. Ontology Extration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bda678-c315-419d-a70c-673a5e07c0e0",
   "metadata": {},
   "source": [
    "### 2.1 Betweenness Centrality Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42d6b8-c3a4-4b21-bd52-874a9649c50d",
   "metadata": {},
   "source": [
    "Run betweenees algorithms with approximation (based on k as number of pivots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02595cc2-9edc-45eb-8eef-2381e456152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(betweeness_path):\n",
    "    with open(betweeness_path, \"r\") as file:\n",
    "        betweenness_centrality_res = json.load(file)\n",
    "else:\n",
    "    betweenness_centrality_res = nx.betweenness_centrality(G_w, backend=\"parallel\", weight=\"weight\", k=500)\n",
    "    with open(betweeness_path, \"w\") as file:\n",
    "        json.dump(betweenness_centrality_res, file)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c4aa9-c430-476c-bc3a-c9f5974b3c26",
   "metadata": {},
   "source": [
    "Tag the edges with respect to the betweeness results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52c469-7894-4c6b-8e19-b6633c82f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v in G_w.edges():\n",
    "    if betweenness_centrality_res[u] == 0 and betweenness_centrality_res[v] == 0:  # Identity tags\n",
    "        nx.set_edge_attributes(G_w, {(u, v):{\"tag\": TAGS[\"I-1\"]}})\n",
    "    else:  # Hierarchy tags\n",
    "        parent = u if betweenness_centrality_res[u] > betweenness_centrality_res[v] else v\n",
    "        child = u if parent != u else v\n",
    "        nx.set_edge_attributes(G_w, {(parent, child):{\"tag\": TAGS[\"H-1\"], \"dir\":(parent, child)}})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ce0fb-fef8-424b-9d1f-9bf81c9a92bf",
   "metadata": {},
   "source": [
    "### 2.2 Betweeness Centrality - Completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264e339-9dc9-4c89-8c41-90f230b93eb9",
   "metadata": {},
   "source": [
    "#### Classifying Phrase Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab2c2d6-5b95-464a-bb12-12d0841beff8",
   "metadata": {},
   "source": [
    "Create a dictionary to store occurrences of phrases categorized by 'name' or 'noun'. A phrase counts as a noun if it has at least one occurrence without a capital letter, otherwise it counts as a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d5b55-9d7b-4141-82bc-5d94a2e9cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(phrase2clean_path):\n",
    "    print(\"missing file!\", phrase2clean_path)\n",
    "else:\n",
    "    with open(phrase2clean_path, \"rb\") as f:\n",
    "        phrase2clean = pickle.load(f)\n",
    "        clean2phrase = {v: [k for k in phrase2clean if phrase2clean[k] == v] for k, v in phrase2clean.items()}\n",
    "        tagged_phrases =  {phrase: \"name\" for phrase in clean2phrase.keys()}\n",
    "        for phrase, occurences in clean2phrase.items():\n",
    "            for occur in occurences:\n",
    "                if occur.islower():\n",
    "                    tagged_phrases[phrase.lower()] = \"noun\"\n",
    "    print(Counter(tagged_phrases.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6612b-d03e-4e59-80a0-353c6b26a930",
   "metadata": {},
   "source": [
    "#### Correcting Directionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea0165-a924-40ad-a4b9-f4206a579f51",
   "metadata": {},
   "source": [
    "Update edge direction in the graph 'G_w' based on the tags of connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd6b0b-f0e0-4e2c-b868-d6d7983fdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v in G_w.edges:\n",
    "    data = G_w.get_edge_data(u, v) \n",
    "    if \"dir\" not in data: # Filter non-hierarchical tags (identity, noise)\n",
    "        continue\n",
    "    if u in tagged_phrases and v in tagged_phrases: \n",
    "        tags = {tagged_phrases[u], tagged_phrases[v]}\n",
    "        curr_dir = data[\"dir\"] \n",
    "        parent, child = curr_dir  # Unpack the current direction\n",
    "        tags_curr_dir = (tagged_phrases[parent], tagged_phrases[child]) \n",
    "        if tags_curr_dir == ('name', 'noun'): \n",
    "            nx.set_edge_attributes(G_w, {(u, v): {\"tag\": TAGS[\"H-2\"], \"dir\": (child, parent)}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003164c9-56ab-4c06-bbef-37f6796e7576",
   "metadata": {},
   "source": [
    "#### Correcting Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa1b1a-f2f6-4ed6-927d-7d1203c9d57a",
   "metadata": {},
   "source": [
    "Make the graph directed and use it to locate name phrases the have children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b606e2d-6ce5-4a17-b90e-9ca88989b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_dir = get_directed_graph(G_w)\n",
    "names_having_children = [ph for ph in tagged_phrases if tagged_phrases[ph] == \"name\" and ph in G_dir and len(list(G_dir.successors(ph))) > 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb474c47-8e70-4665-a642-7f73cf73a01d",
   "metadata": {},
   "source": [
    "For each split node save the relevant edges to modify and the node for removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39773f2-a71b-4540-af69-28e4fc44c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_edges = []\n",
    "parent_edges = []\n",
    "nodes_to_remove = set()\n",
    "\n",
    "k_neighbors = 2\n",
    "\n",
    "for ph in tqdm(names_having_children[:10]):\n",
    "    \n",
    "    children = list(G_dir.successors(ph))\n",
    "    embeds = get_embeds(children)\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=k_neighbors)\n",
    "    knn.fit(embeds)\n",
    "    _, indices = knn.kneighbors(embeds)\n",
    "    \n",
    "    knn_graph = nx.Graph()\n",
    "    for i, child in enumerate(children):\n",
    "        for neighbor_idx in indices[i]:\n",
    "            knn_graph.add_edge(child, children[neighbor_idx])\n",
    "    \n",
    "    partition = community_louvain.best_partition(knn_graph)\n",
    "\n",
    "    concepts = group_strings_by_class(children, list(partition.values()))\n",
    "\n",
    "    common_parents = set(G_dir.predecessors(ph))\n",
    "    \n",
    "    if len(concepts) == 1:  # Case 1: single community detected\n",
    "        for child_group in concepts.values():\n",
    "            for child in child_group:\n",
    "                child_edges.append((ph, child))\n",
    "                \n",
    "                if child in G_dir.nodes:\n",
    "                    common_parents.update(G_dir.predecessors(child))\n",
    "        \n",
    "        for parent in common_parents:\n",
    "            parent_edges.append((parent, ph))\n",
    "    \n",
    "    else:  # Case 2: multiple communities detected\n",
    "        for k, v in concepts.items():\n",
    "            for ind, child in enumerate(v):\n",
    "                curr_ph = f\"{ph} [[[ {k + ind + 1} ]]]\"\n",
    "                child_edges.append((curr_ph, child))\n",
    "                \n",
    "                if child in G_dir.nodes:\n",
    "                    common_parents.update(G_dir.predecessors(child))\n",
    "            \n",
    "            for parent in common_parents:\n",
    "                parent_edges.append((parent, curr_ph))\n",
    "            \n",
    "            common_parents = set(G_dir.predecessors(ph))\n",
    "\n",
    "        nodes_to_remove.add(ph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a84ecf-e57d-4967-b3b2-06929dee8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v in child_edges:\n",
    "    G_w.add_edge(u, v, weight=1.5, tag=TAGS[\"I-2\"])\n",
    "\n",
    "for u, v in parent_edges:\n",
    "    G_w.add_edge(u, v, weight=1.5, tag=TAGS[\"H-1\"])\n",
    "\n",
    "G_w.remove_nodes_from(nodes_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a849750-fc32-46d4-83c8-000e933adabd",
   "metadata": {},
   "source": [
    "## 3. Cleaning Noisy Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b7e3ca-9ef8-4713-9683-89ccb1dcc25f",
   "metadata": {},
   "source": [
    "Calculate the weighted degree for each node in the graph 'G_w', and the total sum of all weighted degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c93940-16f8-4700-baa8-dc131c7311ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_degrees = {node: sum(weight for _, _, weight in G_w.edges(node, data='weight')) for node in G_w.nodes}\n",
    "all_occur = sum(weighted_degrees.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e63c56-be4f-4a95-9f15-04fc7dc06b79",
   "metadata": {},
   "source": [
    "Calculate the PMI for each edge (u, v) in the graph based on weighted degrees and edge weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38d961-89ab-47b0-a305-a13058370346",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi = defaultdict()\n",
    "for u, v in G_w.edges:\n",
    "    w = G_w.get_edge_data(u, v)[\"weight\"]\n",
    "    pmi[(u, v)] = calculate_pmi(weighted_degrees[u], weighted_degrees[v], w, all_occur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317da612-85e5-476e-acea-74b05e7227ec",
   "metadata": {},
   "source": [
    "Assign a noise tag to edges in the graph 'G_w' if their PMI value is less than or equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a25b56-c7b2-41b8-bf22-bed21128af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (u, v), val in pmi.items():\n",
    "    if val <= 0: \n",
    "        nx.set_edge_attributes(G_w, {(u, v): {\"tag\": TAGS[\"N\"]}}) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5d997-22c6-4e13-a8e8-0b8c6be0ca83",
   "metadata": {},
   "source": [
    "## 4. The Resulted Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c30fe-8208-48a3-9a24-01c4176b76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tags_status(G_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f65f0-a85c-4caa-872a-5f34c7bb59f9",
   "metadata": {},
   "source": [
    "Generate a DataFrame from the graph 'G_w' containing the edges, weights, and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757108d-7f27-405c-be81-4930dc2004b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_of_graph(G_w)\n",
    "# df.to_csv(os.path.join(out_path, 'full_graph_tagged.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e29e4a-44cb-4fab-a763-0ed1fe606d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(out_path): \n",
    "    os.makedirs(out_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a26026-f6c7-45ec-a72b-d74526c2c65e",
   "metadata": {},
   "source": [
    "Update concepts in the graph 'G_w' and obtain updated mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a2efd-7f90-4f7a-892b-cb56b2af0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_concepts_updated, phrase2id_updated = update_concepts(G_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1514c-72ec-4717-bd1d-d37ace641be6",
   "metadata": {},
   "source": [
    "Generate a directed acyclic graph (DAG) using the df representing the graph 'G_w'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f1efc-20d0-4001-b6f6-88ebfa8ff7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag, _ = get_dag(df, phrase2id_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155dbae-2f43-4ded-8c24-1a035b98b5e7",
   "metadata": {},
   "source": [
    "Invert the DAG to create a mapping of children to their respective parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13714b8-b1ef-4b28-b7de-51870a7a3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_dag = invert_dag(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ede5f-e377-4ebe-9fee-91aef315017e",
   "metadata": {},
   "source": [
    "Save the resulting ontology files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4a157-4579-4bdb-877a-ae47b5251a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_path, \"dict_concepts_updated.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(dict_concepts_updated, f)\n",
    "with open(os.path.join(out_path, \"phrase2id_updated.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(phrase2id_updated, f)\n",
    "with open(os.path.join(out_path, \"children_updated.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(dag, f)\n",
    "with open(os.path.join(out_path, \"parents_updated.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(inverted_dag, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
